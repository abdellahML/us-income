# Model evaluation challenge - US Income
## Context
Name of the project: Model evaluation challenge - US Income  
Context of the project: BeCode, Li√®ge Campus, AI/Data Operator Bootcamp, February 2021  
Objective of the project: 
- Resolve a machine learning problem.
- Given a preprocessed dataset an RandomForestClassifier, use multiple evaluation metrics to evaluate the performance of the model and tune your hyper-parameters to improve your prediction performance.
- We used the cleaned datasets, `data_train.csv` and `data_test.csv`, that have already been downloaded for you.


### Description
In this project we create a Python script to run, test and improve our Random Forest model.

The power of our prediction model has been evaluated using multiple evaluation metrics such as classification reports, crosstabs, Roc curve evaluation metrics, Matthews correlation coefficient metrics, confusion matrices,..

The improvement of the model has been reached by using the GridSearchCV algorithm and by tunning its parameters.


### Usage
You can run the corresponding functions to tune the hyperparameters and measure the perfomances of each model, respectively.  
Otherwise, you have only to run the `main.py` file which computes and displays evrythink.


### Libraries and environment
`pip install -r requirements.txt`

### Authors

*Abdellah El Ghibzouri, junior AI developer at BeCode;*  

*Christian Melot, junior AI developer at BeCode*
